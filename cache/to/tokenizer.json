{"status": "active", "preferred_support_tool": "", "preferred_support_url": "", "labels": [], "private": false, "creation_date": "2003-04-23", "socialnetworks": [], "tools": [{"sourceforge_group_id": 79549, "mount_point": "summary", "name": "summary", "label": "Summary"}, {"mount_point": "support", "name": "support", "label": "Support"}, {"mount_point": "reviews", "name": "reviews", "label": "Reviews"}, {"mount_point": "wiki", "name": "wiki", "label": "Wiki"}, {"mount_point": "files", "name": "files", "label": "Files"}, {"mount_point": "news", "name": "blog", "label": "News"}, {"mount_point": "code", "name": "cvs", "label": "Code"}], "categories": {"developmentstatus": [], "environment": [], "language": [], "license": [], "database": [], "topic": [], "audience": [], "translation": [], "os": []}, "_id": "5125328434309d4d0ad1bafd", "name": "Sentence and Word Tokenizer", "url": "http://sourceforge.net/p/tokenizer/", "icon_url": null, "video_url": "", "screenshots": [], "summary": "", "short_description": "Sentence and Word Tokenizer tries to solve the simple problem of tokenizing an English text into sentences and words.", "moved_to_url": "", "shortname": "tokenizer", "developers": [{"url": "http://sourceforge.net/u/alexijenks/", "username": "alexijenks", "name": "Jenks Gibbons"}, {"url": "http://sourceforge.net/u/ushaks/", "username": "ushaks", "name": "Usha"}, {"url": "http://sourceforge.net/u/limayes/", "username": "limayes", "name": "sarika"}, {"url": "http://sourceforge.net/u/cs5b/", "username": "cs5b", "name": "Christian Seifert"}, {"url": "http://sourceforge.net/u/jamay/", "username": "jamay", "name": "Jose Chavez"}], "external_homepage": "http://tokenizer.sourceforge.net"}
