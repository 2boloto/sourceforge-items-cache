{"_id": "5125328434309d4d0ad1bafd", "categories": {"audience": [], "database": [], "developmentstatus": [], "environment": [], "language": [], "license": [], "os": [], "topic": [], "translation": []}, "creation_date": "2003-04-23", "developers": [{"name": "Jenks Gibbons", "url": "http://sourceforge.net/u/alexijenks/", "username": "alexijenks"}, {"name": "Usha", "url": "http://sourceforge.net/u/ushaks/", "username": "ushaks"}, {"name": "sarika", "url": "http://sourceforge.net/u/limayes/", "username": "limayes"}, {"name": "Christian Seifert", "url": "http://sourceforge.net/u/cs5b/", "username": "cs5b"}, {"name": "Jose Chavez", "url": "http://sourceforge.net/u/jamay/", "username": "jamay"}], "external_homepage": "http://tokenizer.sourceforge.net", "icon_url": null, "labels": [], "moved_to_url": "", "name": "Sentence and Word Tokenizer", "preferred_support_tool": "", "preferred_support_url": "", "private": false, "screenshots": [], "short_description": "Sentence and Word Tokenizer tries to solve the simple problem of tokenizing an English text into sentences and words.", "shortname": "tokenizer", "socialnetworks": [], "status": "active", "summary": "", "tools": [{"label": "Summary", "mount_point": "summary", "name": "summary", "sourceforge_group_id": 79549}, {"label": "Support", "mount_point": "support", "name": "support"}, {"label": "Reviews", "mount_point": "reviews", "name": "reviews"}, {"label": "Wiki", "mount_point": "wiki", "name": "wiki"}, {"label": "Files", "mount_point": "files", "name": "files"}, {"label": "News", "mount_point": "news", "name": "blog"}, {"label": "Code", "mount_point": "code", "name": "cvs"}], "url": "http://sourceforge.net/p/tokenizer/", "video_url": ""}
