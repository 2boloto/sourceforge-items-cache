{"status": "active", "external_homepage": "http://gerritjvv.github.io/glue/", "developers": [{"url": "http://sourceforge.net/u/gerritjvv/", "username": "gerritjvv", "name": "Gerrit Jansen van Vuuren"}], "screenshots": [], "name": "glue", "preferred_support_tool": "_members", "preferred_support_url": "", "icon_url": null, "labels": ["hadoop", "glue", "workflows", "hdfs", "pig", "mapreduce", "cascalog", "hive", "mysql"], "video_url": "", "private": false, "creation_date": "2013-07-03", "url": "http://sourceforge.net/p/glueworkflows/", "socialnetworks": [{"accounturl": "", "socialnetwork": "Twitter"}], "short_description": "Glue is a job execution engine, written in Java and Groovy. workflows are written in Groovy DSL (simple statements) and use pre-developed modules to interact with external resources e.g. DBs, Hadoop, Netezza, FTP etc.\r\n\r\nIn Glue one of the main features and design goals is to always abstract configuration away from functionality. This means that no more hardcoded IPs, UserIDs and Passwords spread over 10s of hundreds of bash/python scripts.\r\n\r\nScripts written for one environment can be easily ported to another because the configuration is done outside of each workflow.\r\n\r\nHow is this done?\r\n\r\nEach Module has a configuration section in the /opt/glue/conf/workflow_modules.grooy where data like hosts, ips, usernames etc are placed. This configuration is loaded and provided to the Module before starting each workflow.\r\n\r\nConfigurations can be changed dynamically and are re-read before each workflow run, such that no restart is required.\r\n", "moved_to_url": "", "shortname": "glueworkflows", "_id": "51d3f879a02bb13036d581cd", "tools": [{"sourceforge_group_id": 1890868, "mount_point": "summary", "name": "summary", "label": "Summary"}, {"mount_point": "files", "name": "files", "label": "Files"}, {"mount_point": "reviews", "name": "reviews", "label": "Reviews"}, {"mount_point": "support", "name": "support", "label": "Support"}, {"mount_point": "wiki", "name": "wiki", "label": "Wiki"}, {"mount_point": "discussion", "name": "discussion", "label": "Discussion"}, {"mount_point": "blog", "name": "blog", "label": "Blog"}], "summary": "BigData Workflow Engine for Hadoop, Hbase, Netezza, Pig, Hive ...", "categories": {"developmentstatus": [], "topic": [], "language": [], "license": [{"fullpath": "License :: OSI-Approved Open Source :: Apache License V2.0", "shortname": "apache2", "fullname": "Apache License V2.0", "id": 401}], "database": [], "environment": [], "audience": [], "translation": [], "os": [{"fullpath": "Operating System :: Grouping and Descriptive Categories :: All POSIX (Linux/BSD/UNIX-like OSes)", "shortname": "posix", "fullname": "All POSIX (Linux/BSD/UNIX-like OSes)", "id": 200}]}}
